---
title: "RE Sparsity"
output: github_document
vignette: >
  %\VignetteIndexEntry{RE Sparsity}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Many stock assessment models in the US have historically used a deviations approach 
when estimating time varying process variability in parameters such as recruitment. In this 
approach a base value and a devation value are estimated and combined to produce
each parameter estimate. Penalties such as a sum to zero constraint and 
variance estimate are then included in the likelihood component of the deviations.
While simple to interpret and implement, this approach results in a dense convariance
matrix of correlations between the base and devation estimates. This was not a 
significant concern for historic assessment models that treated deviations as 
penalized fixed effects and therefore didn't need to calculate second order partial
derivatives during the parameter estimation phase. 

When considering the implementation of random effects to model process variability 
in FIMS we had to consider the impact of using the historic deviations approach 
on model covariance sparcity. In order to integrate out random effect uncertainty
during model optimization  the lapace approximation used in  to 
integrate out random effect uncertainty during the model optimization.  is estimated and Preliminary research indicates 
the random effects on the process (non-centered) will be equivalent mathematically 
but more sparse, leading to more computational efficiency when the process is an AR1 model.

With repect to time series analysis, the non-centered approach will result in a 
sparse covariance matrix. When estimating random effects via the Laplace approximation, 
compuational gains will be evident in the non-centered over centered approach due 
to the speed up in the Laplace approzimation calculations that are a direct result 
of this sparsity. 

This analysis looks at both a simple AR1 time series and a simple catch-at-age 
stock assessment model with an AR1 process in recruitment. Both analyses demonstrate 
that model results are the same despite the parameterization used, however, the 
non-centered approach results in computational efficiency gains. While speed up 
for a single AR1 process is minimal, more complex stock assessments currently 
include AR1 random effects in multiple processes (e.g. numbers at age, maturity, 
mortality, etc.). The computational cost of fitting the centered approach scales 
significantly, warranting the need to adopt the more computationally efficient 
non-centered parameterization

## Methods

We evaluated a simple time series model with an AR1 process, 
$\lambda_{t} = p * \lambda_{t-1} + \epsilon_{t}$, $\lambda_t$ is the AR1 process 
at time $t$ and where $p$ is the AR1 correlation parameter, using two parameterizations:

1. process approach: $\lambda_{t} \sim N(p * \lambda_{t-1}, \sigma^{2}_{\lambda})$
2. deviations approach: $\lambda_{t} = p * \lambda_{t-1} + \sigma * z_{t}, z_{t} \sim N(0,1)$

Under a random effects model, the process approach puts the random effect on $\lambda_{t}$, while in
the deviations approach, the random effect is applied to $z_{t}$. [Text on why the process approach is 
sparse and why this matters for the Laplace approximation]. 

We also compared these two parameterization in a simple catch-at-age stock assessment
by modifying the software package, babySAM. [Text descibing babySAM and the modifications]

We compared model results and looked at computational speed.  

## Results

The difference between all fixed effect parameters in both models were the same. 
In addition, there was no difference between recruitment from the different 
parameterizarions in the simple catch-at-age model. 

