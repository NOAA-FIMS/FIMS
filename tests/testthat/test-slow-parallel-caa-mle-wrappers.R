# Instructions ----
#' This file follows the format generated by FIMS:::use_testthat_template().
#' Necessary tests include input and output (IO) correctness [IO
#' correctness], edge-case handling [Edge handling], and built-in errors and
#' warnings [Error handling]. See `?FIMS:::use_testthat_template` for more
#' information. Every test should have a @description tag, which can span
#' multiple lines, that will be used in the bookdown report of the results from
#' {testthat}.

# Run FIMS in serial and parallel ----
# This test demonstrates how to run the FIMS model in both serial and parallel
# modes. The parallel execution uses {snowfall} to parallelize the tasks across
# multiple CPU cores.

# Ensure the latest precompiled version of FIMS is installed in R before
# running devtools. To do this, either run:
# - devtools::install() followed by devtools::test(), or
# - devtools::check()

## Setup ----
#' @description Skip the test unless explicitly enabled for heavy integration testing
testthat::skip_if_not(
  testthat:::env_var_is_true("RUN_SLOW_TESTS"),
  message = "Skipping: RUN_SLOW_TESTS is not set to true."
)

# Load the model comparison operating model data from the fixtures folder
load(testthat::test_path("fixtures", "integration_test_data.RData"))

# Simulation configuration
# We run a subset of simulations to verify that the FIMS estimation wrapper
# produces identical results regardless of the execution mode (serial vs parallel).
#
# Memory Management Note:
# R crashes when attempting to run 10 models in parallel with the error
# `cannot allocate vector of size 1.4 Gigabyte`. According to Gemini, the error indicates
# that during the serialization or unserialization of the data between the main
# R process and the worker nodes, R requested more memory than the system or the
# R environment allowed. This may be related to the size of the tibble being
# passed around. We limit sim_num to 4 to ensure the tests pass on CI.
sim_num <- 4

# Create the initial FIMS data and default parameter structures,
# then map through the simulation iterations to apply iteration-specific
# values from the operating model.
data_age_length_comp <- FIMSFrame(data1)
default_parameters <- create_default_configurations(
  data = data_age_length_comp
) |>
  create_default_parameters(
    data = data_age_length_comp
  )

modified_parameters <- purrr::map(1:sim_num, \(iter_id) {
  default_parameters |>
    tidyr::unnest(cols = data) |>
    # Update log_Fmort input values for Fleet1
    dplyr::rows_update(
      tibble::tibble(
        fleet_name = "fleet1",
        label = "log_Fmort",
        time = 1:30,
        value = log(om_output_list[[iter_id]][["f"]]),
      ),
      by = c("fleet_name", "label", "time")
    ) |>
    # Update selectivity parameters and log_q for survey1
    dplyr::rows_update(
      tibble::tibble(
        fleet_name = "survey1",
        label = c("inflection_point", "slope", "log_q"),
        value = c(1.5, 2, log(om_output_list[[iter_id]][["survey_q"]][["survey1"]]))
      ),
      by = c("fleet_name", "label")
    ) |>
    # Update log_devs in the Recruitment module (time steps 2â€“30)
    dplyr::rows_update(
      tibble::tibble(
        label = "log_devs",
        time = 2:30,
        value = om_input_list[[iter_id]][["logR.resid"]][-1],
        # TODO: integration tests fail after setting recruitment log_devs all estimable.
        # We need to debug the issue, then change constant to fixed_effects.
        # estimation_type = "fixed_effects"
        estimation_type = "constant"
      ),
      by = c("label", "time")
    ) |>
    # Update log_sd for log_devs in the Recruitment module
    dplyr::rows_update(
      tibble::tibble(
        module_name = "Recruitment",
        label = "log_sd",
        value = om_input_list[[iter_id]][["logR_sd"]]
      ),
      by = c("module_name", "label")
    ) |>
    # Update inflection point and slope parameters in the Maturity module
    dplyr::rows_update(
      tibble::tibble(
        module_name = "Maturity",
        label = c("inflection_point", "slope"),
        value = c(
          om_input_list[[iter_id]][["A50.mat"]],
          om_input_list[[iter_id]][["slope.mat"]]
        )
      ),
      by = c("module_name", "label")
    ) |>
    # Update log_init_naa values in the Population module
    dplyr::rows_update(
      tibble::tibble(
        label = "log_init_naa",
        age = 1:12,
        value = log(om_output_list[[iter_id]][["N.age"]][1, ])
      ),
      by = c("label", "age")
    )
})

# Serial execution
# Establish the baseline "ground truth" results using standard purrr::map.
estimation_results_serial <- purrr::map(1:sim_num, \(iter_id) {
  setup_and_run_FIMS_with_wrappers(
    iter_id = iter_id,
    om_input_list = om_input_list,
    om_output_list = om_output_list,
    em_input_list = em_input_list,
    estimation_mode = TRUE,
    modified_parameters = modified_parameters
  )
})

## IO correctness ----
test_that("Run FIMS in parallel using {snowfall}", {
  # Parallel Initialization
  core_num <- 2
  # suppressWarnings is used to hide the 'Unknown option --file' warning
  # triggered by snowfall's command-line scanner during Rscript execution.
  suppressWarnings(snowfall::sfInit(parallel = TRUE, cpus = core_num))

  # Ensure the FIMS package and required objects are available on all worker nodes.
  snowfall::sfLibrary(FIMS)

  # Parallel Execution
  results_parallel <- snowfall::sfLapply(
    1:sim_num,
    setup_and_run_FIMS_with_wrappers,
    om_input_list,
    om_output_list,
    em_input_list,
    TRUE,
    FALSE,
    modified_parameters
  )

  snowfall::sfStop()

  # Comparison of results:
  # Verify that spawning biomass values from both runs are equivalent.
  sb_parallel <- purrr::map(
    results_parallel,
    \(x) get_estimates(x) |>
      dplyr::filter(label == "spawning_biomass") |>
      dplyr::pull(estimated)
  )

  sb_serial <- purrr::map(
    estimation_results_serial,
    \(x) get_estimates(x) |>
      dplyr::filter(label == "spawning_biomass") |>
      dplyr::pull(estimated)
  )
  #' @description Test that spawning biomass values from parallel runs equal those from serial runs. We apply a tolerance of 1e-5 to floating-point comparison.
  expect_equal(sb_parallel, sb_serial, tolerance = 1e-5)

  parameters_parallel <- purrr::map(
    results_parallel,
    \(x) get_estimates(x) |>
      dplyr::filter(estimation_type == "fixed_effects") |>
      dplyr::pull(estimated)
  )
  parameters_serial <- purrr::map(
    estimation_results_serial,
    \(x) get_estimates(x) |>
      dplyr::filter(estimation_type == "fixed_effects") |>
      dplyr::pull(estimated)
  )
  #' @description Test that parameter estimates from parallel runs equal those from serial runs. We apply a tolerance of 1e-5 to floating-point comparison.
  expect_equal(parameters_parallel, parameters_serial, tolerance = 1e-5)

  jnll_parallel <- purrr::map(
    results_parallel,
    \(x) get_report(x)[["jnll"]]
  )
  jnll_serial <- purrr::map(
    estimation_results_serial,
    \(x) get_report(x)[["jnll"]]
  )
  #' @description Test that total NLL values from parallel runs equal those from serial runs.
  expect_equal(jnll_parallel, jnll_serial)
})

## Edge handling ----
# No edge cases to test.

## Error handling ----
# No built-in errors to test.
